name: IMDB_DEV_train_LlamBert
name_fields: !!python/tuple [model_name]
neptune_logging: True
train_fpath: /home/projects/DeepNeurOntology/IMDB_data/BERT_inputs/DEV/Llama_output_train/train.json
dev_fpath: /home/projects/DeepNeurOntology/IMDB_data/BERT_inputs/DEV/Llama_output_train/dev.json
test_fpath: /home/projects/DeepNeurOntology/IMDB_data/BERT_inputs/DEV/Gold/test.json
model_name: roberta-large
batch_size: 16
num_epochs: 10
max_len: 512
model_save_location: null
reduce_lines_for_testing: false # uses only 100 sentences to train model
lr: 0.000001
mislabel_percent: 0  # 0 if you don't want mislabeled data in the train set (not in val or test) 1 -> 1%, 2 -> 2% so on