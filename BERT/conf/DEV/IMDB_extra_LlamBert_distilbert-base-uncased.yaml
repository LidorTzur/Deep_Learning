name: IMDB_DEV_extra_LlamBert
name_fields: !!python/tuple [model_name]
neptune_logging: True
train_fpath: /home/projects/DeepNeurOntology/IMDB_data/BERT_inputs/Llama_output_extra.json
dev_fpath: /home/projects/DeepNeurOntology/IMDB_data/BERT_inputs/DEV/Llama_output_train/dev.json
test_fpath: /home/projects/DeepNeurOntology/IMDB_data/BERT_inputs/DEV/Gold/test.json
model_name: distilbert-base-uncased
batch_size: 16
num_epochs: 5
max_len: 512
model_save_location: /home/projects/DeepNeurOntology/model_checkpoints/Extra_llama_distilbert-base-uncased/
reduce_lines_for_testing: false # uses only 100 sentences to train model
lr: 0.000001
mislabel_percent: 0  # 0 if you don't want mislabeled data in the train set (not in val or test) 1 -> 1%, 2 -> 2% so on